{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8230c11",
   "metadata": {},
   "source": [
    "# Spark Dataframe Operations\n",
    "\n",
    "<ol style = \"type:1\">\n",
    "    <li><a href = \"#ref\">References</a></li>\n",
    "</ol>\n",
    "\n",
    "We begin by creating a Spark session, then read `appl_stock.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58375dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61934066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output hidden\n",
    "spark = SparkSession.builder.appName(\"ops\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8dd6246",
   "metadata": {},
   "source": [
    "When reading `.csv` files, we have the option to infer schema. Note that this is not the can with `.json` files. We also set `header = True`, meaning that the first row of the `.csv` file is the column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e9a9227",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"appl_stock.csv\", \n",
    "                    inferSchema = True, \n",
    "                    header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00f780db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      " |-- Adj Close: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0453b6e",
   "metadata": {},
   "source": [
    "If we want to see what the actual dataframe looks like,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c84a4bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+----------+------------------+----------+---------+------------------+\n",
      "|               Date|      Open|      High|               Low|     Close|   Volume|         Adj Close|\n",
      "+-------------------+----------+----------+------------------+----------+---------+------------------+\n",
      "|2010-01-04 00:00:00|213.429998|214.499996|212.38000099999996|214.009998|123432400|         27.727039|\n",
      "|2010-01-05 00:00:00|214.599998|215.589994|        213.249994|214.379993|150476200|27.774976000000002|\n",
      "|2010-01-06 00:00:00|214.379993|    215.23|        210.750004|210.969995|138040000|27.333178000000004|\n",
      "+-------------------+----------+----------+------------------+----------+---------+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ced0a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+----------+------------------+----------+---------+------------------+\n",
      "|               Date|      Open|      High|               Low|     Close|   Volume|         Adj Close|\n",
      "+-------------------+----------+----------+------------------+----------+---------+------------------+\n",
      "|2010-01-04 00:00:00|213.429998|214.499996|212.38000099999996|214.009998|123432400|         27.727039|\n",
      "|2010-01-05 00:00:00|214.599998|215.589994|        213.249994|214.379993|150476200|27.774976000000002|\n",
      "|2010-01-06 00:00:00|214.379993|    215.23|        210.750004|210.969995|138040000|27.333178000000004|\n",
      "+-------------------+----------+----------+------------------+----------+---------+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094c2249",
   "metadata": {},
   "source": [
    "Say we want to grab all the data that had a closing price less than \\\\$500. We can pass in SQL syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6ae4cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+----------+------------------+------------------+---------+------------------+\n",
      "|               Date|      Open|      High|               Low|             Close|   Volume|         Adj Close|\n",
      "+-------------------+----------+----------+------------------+------------------+---------+------------------+\n",
      "|2010-01-04 00:00:00|213.429998|214.499996|212.38000099999996|        214.009998|123432400|         27.727039|\n",
      "|2010-01-05 00:00:00|214.599998|215.589994|        213.249994|        214.379993|150476200|27.774976000000002|\n",
      "|2010-01-06 00:00:00|214.379993|    215.23|        210.750004|        210.969995|138040000|27.333178000000004|\n",
      "|2010-01-07 00:00:00|    211.75|212.000006|        209.050005|            210.58|119282800|          27.28265|\n",
      "|2010-01-08 00:00:00|210.299994|212.000006|209.06000500000002|211.98000499999998|111902700|         27.464034|\n",
      "+-------------------+----------+----------+------------------+------------------+---------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(\"Close < 500\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11f9cc1",
   "metadata": {},
   "source": [
    "We can then combine it with `.select()` method, then show the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f72c238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      Open|\n",
      "+----------+\n",
      "|213.429998|\n",
      "|214.599998|\n",
      "|214.379993|\n",
      "|    211.75|\n",
      "|210.299994|\n",
      "+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(\"Close < 500\").select(\"Open\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71faa01",
   "metadata": {},
   "source": [
    "To show multiple columns, pass in a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1939c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|      Open|             Close|\n",
      "+----------+------------------+\n",
      "|213.429998|        214.009998|\n",
      "|214.599998|        214.379993|\n",
      "|214.379993|        210.969995|\n",
      "|    211.75|            210.58|\n",
      "|210.299994|211.98000499999998|\n",
      "+----------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(\"Close < 500\").select([\"Open\", \"Close\"]).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ed8407",
   "metadata": {},
   "source": [
    "This looks similar to SQL operators, except that you need to make sure you are calling the entire column with the dataframe, using the `df[column_name]` format. For instance,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c27d65e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+----------+------------------+------------------+---------+------------------+\n",
      "|               Date|      Open|      High|               Low|             Close|   Volume|         Adj Close|\n",
      "+-------------------+----------+----------+------------------+------------------+---------+------------------+\n",
      "|2010-01-04 00:00:00|213.429998|214.499996|212.38000099999996|        214.009998|123432400|         27.727039|\n",
      "|2010-01-05 00:00:00|214.599998|215.589994|        213.249994|        214.379993|150476200|27.774976000000002|\n",
      "|2010-01-06 00:00:00|214.379993|    215.23|        210.750004|        210.969995|138040000|27.333178000000004|\n",
      "|2010-01-07 00:00:00|    211.75|212.000006|        209.050005|            210.58|119282800|          27.28265|\n",
      "|2010-01-08 00:00:00|210.299994|212.000006|209.06000500000002|211.98000499999998|111902700|         27.464034|\n",
      "+-------------------+----------+----------+------------------+------------------+---------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df[\"Close\"] < 500).select(\"Volume\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd80f190",
   "metadata": {},
   "source": [
    "Next, let us walk through an example of filtering based off multiple conditions. Suppose we want to grab all the rows that have a closing price less than \\\\$200, and an open price greater than \\\\$200. There are two common mistakes when doing this.\n",
    "\n",
    "**Mistake No. 1: `df.filter(df[\"Close\"] < 200 and df[\"Open\"] > 200).show(5)`**\n",
    "\n",
    "We will then get an error message saying that we need to replace the `and` operator with the bitwise-and `&` operator.\n",
    "\n",
    "**Mistake No. 2: `df.filter(df[\"Close\"] < 200 & df[\"Open\"] > 200).show(5)`**\n",
    "\n",
    "This is a tricky error---we will get a `Py4JError` with insufficient cludes.\n",
    "\n",
    "It turns out we need to distinctly separate the two conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34e55cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+----------+----------+----------+---------+------------------+\n",
      "|               Date|              Open|      High|       Low|     Close|   Volume|         Adj Close|\n",
      "+-------------------+------------------+----------+----------+----------+---------+------------------+\n",
      "|2010-01-22 00:00:00|206.78000600000001|207.499996|    197.16|    197.75|220441900|         25.620401|\n",
      "|2010-01-28 00:00:00|        204.930004|205.500004|198.699995|199.289995|293375600|25.819922000000002|\n",
      "|2010-01-29 00:00:00|        201.079996|202.199995|190.250002|192.060003|311488100|         24.883208|\n",
      "+-------------------+------------------+----------+----------+----------+---------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter( (df[\"Close\"] < 200) & (df[\"Open\"] > 200) ).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e95b95",
   "metadata": {},
   "source": [
    "Negation is similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "590d2862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+----------+------------------+----------+---------+------------------+\n",
      "|               Date|              Open|      High|               Low|     Close|   Volume|         Adj Close|\n",
      "+-------------------+------------------+----------+------------------+----------+---------+------------------+\n",
      "|2010-02-01 00:00:00|192.36999699999998|     196.0|191.29999899999999|194.729998|187469100|         25.229131|\n",
      "|2010-02-02 00:00:00|        195.909998|196.319994|193.37999299999998|195.859997|174585600|25.375532999999997|\n",
      "|2010-02-03 00:00:00|        195.169994|200.200003|        194.420004|199.229994|153832000|25.812148999999998|\n",
      "|2010-02-04 00:00:00|        196.730003|198.370001|        191.570005|192.050003|189413000|         24.881912|\n",
      "|2010-02-05 00:00:00|192.63000300000002|     196.0|        190.850002|195.460001|212576700|25.323710000000002|\n",
      "+-------------------+------------------+----------+------------------+----------+---------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter( (df[\"Close\"] < 200) & ~(df[\"Open\"] > 200) ).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfba592",
   "metadata": {},
   "source": [
    "**What date was the low price \\\\$197.16?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e1fb745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+----------+------+------+---------+---------+\n",
      "|               Date|              Open|      High|   Low| Close|   Volume|Adj Close|\n",
      "+-------------------+------------------+----------+------+------+---------+---------+\n",
      "|2010-01-22 00:00:00|206.78000600000001|207.499996|197.16|197.75|220441900|25.620401|\n",
      "+-------------------+------------------+----------+------+------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df[\"Low\"] == 197.16).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7051c92",
   "metadata": {},
   "source": [
    "If we use `.collect()` instead,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4048fd06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Date=datetime.datetime(2010, 1, 22, 0, 0), Open=206.78000600000001, High=207.499996, Low=197.16, Close=197.75, Volume=220441900, Adj Close=25.620401)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter(df[\"Low\"] == 197.16).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d4752b",
   "metadata": {},
   "source": [
    "then we get back a list of row objects, meaning we can save this result. In practice, collecting happens more often than showing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b1b02d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = df.filter(df[\"Low\"] == 197.16).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "edf23f2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Date=datetime.datetime(2010, 1, 22, 0, 0), Open=206.78000600000001, High=207.499996, Low=197.16, Close=197.75, Volume=220441900, Adj Close=25.620401)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b6900c",
   "metadata": {},
   "source": [
    "Rows can be turned into dictionaries. In fact, they have a lot of methods available to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1301af9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "row = result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "59427524",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Date': datetime.datetime(2010, 1, 22, 0, 0),\n",
       " 'Open': 206.78000600000001,\n",
       " 'High': 207.499996,\n",
       " 'Low': 197.16,\n",
       " 'Close': 197.75,\n",
       " 'Volume': 220441900,\n",
       " 'Adj Close': 25.620401}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row.asDict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903d7d39",
   "metadata": {},
   "source": [
    "## <a name = \"ref\">References</a>\n",
    "\n",
    "<ol style = \"type:1\">\n",
    "    <li>Jose Portilla. Spark and Python for Big Data with PySpark.</li>\n",
    "    <li>Apache Spark. <a href = \"https://spark.apache.org/docs/latest/api/python/\">https://spark.apache.org/docs/latest/api/python/</a>.</li>\n",
    "</ol>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
