{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce47ece2",
   "metadata": {},
   "source": [
    "# Groupby and Aggregate Functions on Spark Dataframe\n",
    "\n",
    "## Table of Content\n",
    "\n",
    "<ol style = \"type:1\">\n",
    "    <li><a href = \"#groupbyagg\"><code>`groupBy`</code> and <code>`agg`</code></a></li>\n",
    "    <li><a href = \"#importfuncfromspark\">Importing Functions from Spark</a></li>\n",
    "    <li><a href = \"#sorting\">Sorting</a></li>\n",
    "    <li><a href = \"#ref\">References</a></li>\n",
    "</ol>\n",
    "\n",
    "## <a name = \"groupbyagg\">`groupBy` and `agg`</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90aaccc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d457ba5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output hidden\n",
    "spark = SparkSession.builder.appName(\"aggs\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b50de12c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"sales_info.csv\", \n",
    "                    inferSchema = True, \n",
    "                    header = True\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7599399",
   "metadata": {},
   "source": [
    "Let's see what the data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0132bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+\n",
      "|Company| Person|Sales|\n",
      "+-------+-------+-----+\n",
      "|   GOOG|    Sam|200.0|\n",
      "|   GOOG|Charlie|120.0|\n",
      "|   GOOG|  Frank|340.0|\n",
      "|   MSFT|   Tina|600.0|\n",
      "|   MSFT|    Amy|124.0|\n",
      "+-------+-------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52a23184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Company: string (nullable = true)\n",
      " |-- Person: string (nullable = true)\n",
      " |-- Sales: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224d0a49",
   "metadata": {},
   "source": [
    "To group together by the `Company` column, we call the `.groupBy()` method. `df.groupBy(column_name)` is a `group.GroupedData` object; and Python shows you its memory location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d130177",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.group.GroupedData at 0x7f2da815ca00>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupBy(\"Company\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ca2918",
   "metadata": {},
   "source": [
    "A vareity of methods can be called on this `GroupedData` object, such as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20e2e9ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Company: string, avg(Sales): double]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupBy(\"Company\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e963b676",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|Company|       avg(Sales)|\n",
      "+-------+-----------------+\n",
      "|   APPL|            370.0|\n",
      "|   GOOG|            220.0|\n",
      "|     FB|            610.0|\n",
      "|   MSFT|322.3333333333333|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"Company\").mean().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8504e757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|Company|max(Sales)|\n",
      "+-------+----------+\n",
      "|   APPL|     750.0|\n",
      "|   GOOG|     340.0|\n",
      "|     FB|     870.0|\n",
      "|   MSFT|     600.0|\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"Company\").max().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d579fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|Company|sum(Sales)|\n",
      "+-------+----------+\n",
      "|   APPL|    1480.0|\n",
      "|   GOOG|     660.0|\n",
      "|     FB|    1220.0|\n",
      "|   MSFT|     967.0|\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"Company\").sum().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d2e5dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|Company|count|\n",
      "+-------+-----+\n",
      "|   APPL|    4|\n",
      "|   GOOG|    3|\n",
      "|     FB|    2|\n",
      "|   MSFT|    3|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"Company\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22696e28",
   "metadata": {},
   "source": [
    "etc. Now, not all methods need a `.groupBy()`-call. A generalised `agg` method can be called to aggregate across all rows in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c9be662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|sum(Sales)|\n",
      "+----------+\n",
      "|    4327.0|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.agg({\"Sales\": \"sum\"}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e2190c",
   "metadata": {},
   "source": [
    "This returns the sum of all the sales in the dataframe. We can also have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc206f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|max(Sales)|\n",
      "+----------+\n",
      "|     870.0|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.agg({\"Sales\": \"max\"}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ed1037",
   "metadata": {},
   "source": [
    "This dictionary syntax also works on `groupBy` as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4fb2ede9",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_data = df.groupBy(\"Company\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f12978be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|Company|max(Sales)|\n",
      "+-------+----------+\n",
      "|   APPL|     750.0|\n",
      "|   GOOG|     340.0|\n",
      "|     FB|     870.0|\n",
      "|   MSFT|     600.0|\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "group_data.agg({\"Sales\": \"max\"}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b523d6",
   "metadata": {},
   "source": [
    "## <a name = \"importfuncfromspark\">Importing Functions from Spark</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e6b6f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import countDistinct, avg, stddev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2452316b",
   "metadata": {},
   "source": [
    "We can combine these functions with a `.select()` call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1b681cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|       avg(Sales)|\n",
      "+-----------------+\n",
      "|360.5833333333333|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(avg(\"Sales\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878b1e67",
   "metadata": {},
   "source": [
    "We can change the title `avg(Sales)` with alias. Pay attention that `.alias()` and the function should be inside the same parentheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "74e8a368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|    Average Sales|\n",
      "+-----------------+\n",
      "|360.5833333333333|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(avg(\"Sales\").alias(\"Average Sales\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07743b2e",
   "metadata": {},
   "source": [
    "Let us select the standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3327fc4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|stddev_samp(Sales)|\n",
      "+------------------+\n",
      "|250.08742410799007|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(stddev(\"Sales\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7266952",
   "metadata": {},
   "source": [
    "This is a lengthy number, and we can make it look nicer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "73dadb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import format_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "918c2063",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_std = df.select(stddev(\"Sales\").alias(\"std\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "24c31787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|               std|\n",
      "+------------------+\n",
      "|250.08742410799007|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_std.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b5ac147c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|   std|\n",
      "+------+\n",
      "|250.09|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_std.select(format_number(\"std\", 2).alias(\"std\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bbec6e",
   "metadata": {},
   "source": [
    "## <a href = \"sorting\">Sorting</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e55a3597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+\n",
      "|Company| Person|Sales|\n",
      "+-------+-------+-----+\n",
      "|   GOOG|Charlie|120.0|\n",
      "|   MSFT|    Amy|124.0|\n",
      "|   APPL|  Linda|130.0|\n",
      "|   GOOG|    Sam|200.0|\n",
      "|   MSFT|Vanessa|243.0|\n",
      "|   APPL|   John|250.0|\n",
      "|   GOOG|  Frank|340.0|\n",
      "|     FB|  Sarah|350.0|\n",
      "|   APPL|  Chris|350.0|\n",
      "|   MSFT|   Tina|600.0|\n",
      "+-------+-------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(\"Sales\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a101315",
   "metadata": {},
   "source": [
    "Decending is tricky, since we need to pass in the actual column than the column's name, and call `.desc()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "be4caecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+\n",
      "|Company| Person|Sales|\n",
      "+-------+-------+-----+\n",
      "|     FB|   Carl|870.0|\n",
      "|   APPL|   Mike|750.0|\n",
      "|   MSFT|   Tina|600.0|\n",
      "|     FB|  Sarah|350.0|\n",
      "|   APPL|  Chris|350.0|\n",
      "|   GOOG|  Frank|340.0|\n",
      "|   APPL|   John|250.0|\n",
      "|   MSFT|Vanessa|243.0|\n",
      "|   GOOG|    Sam|200.0|\n",
      "|   APPL|  Linda|130.0|\n",
      "+-------+-------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(df[\"Sales\"].desc()).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c115cb",
   "metadata": {},
   "source": [
    "## <a name = \"ref\">References</a>\n",
    "\n",
    "<ol style = \"type:1\">\n",
    "    <li>Jose Portilla. Spark and Python for Big Data with PySpark.</li>\n",
    "    <li>Apache Spark. <a href = \"https://spark.apache.org/docs/latest/api/python/\">https://spark.apache.org/docs/latest/api/python/</a>.</li>\n",
    "</ol>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
