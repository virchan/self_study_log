\documentclass[12pt,letterpaper,reqno]{amsart}


\include{Apackages} %this loads the packages


\makeindex

\author{Virgil Chan}
\title{Casella-Berger \\ Statistical Inference Solution: \\ Chapter 6}
\date{September 14, 2022}


\usetikzlibrary{patterns,positioning,arrows,chains,matrix,positioning,scopes} %options for tikzpicture

\addbibresource{bibliography.bib} %put your reference here


\makeatletter
\patchcmd{\@maketitle}
  {\ifx\@empty\@dedicatory}
  {\ifx\@empty\@date \else {\vskip3ex \centering\footnotesize\@date\par\vskip1ex}\fi
   \ifx\@empty\@dedicatory}
  {}{}
\patchcmd{\@adminfootnotes}
  {\ifx\@empty\@date\else \@footnotetext{\@setdate}\fi}
  {}{}{}
\makeatother

\include{Atheoremsenvironments}

\pdfpagewidth 8.5in
\pdfpageheight 11in

    
\include{Acommand}

\makeatletter
\tikzset{join/.code=\tikzset{after node path={%
\ifx\tikzchainprevious\pgfutil@empty\else(\tikzchainprevious)%
edge[every join]#1(\tikzchaincurrent)\fi}}}

\makeatother

%\tikzset{>=stealth',every on chain/.append style={join},
%        every join/.style={->}}

\newlength{\parindentsave}\setlength{\parindentsave}{\parindent}

% \everymath{\displaystyle}

\numberwithin{equation}{subsection} 

\let\emptyset\varnothing

\hypersetup{colorlinks,citecolor=blue,linkcolor=blue}

\declaretheorem[numberwithin=section, shaded={rulecolor=black,
rulewidth=0.5pt, bgcolor={rgb}{1,1,1}}]{Theorem}

%\doublespacing

\setcounter{tocdepth}{4}

\everymath{\displaystyle}

\begin{document}
\maketitle

\tableofcontents

\newpage
\section{Exercise 6.1}

We have

\begin{align*}
    \fun{f_X}{\conditbar{x}{\sigma^2}} &= \Gaussianpdf{x}{\sigma} \\
                                       &= \Gaussianpdf{\Abs{x}}{\sigma} \\
                                       &= \fun{f_{X}}{\conditbar{\Abs{x}}{\sigma^2}} \cdot 1.
\end{align*}
Therefore, by \cite[Factorization Theorem 6.2.6 on page 276]{Berger-Casella}, the variable $\Abs{X}$ is a sufficient statistic for $\sigma^2$.

\newpage
\section{Exercise 6.2}

The joint distribution is given by

\begin{align*}
    \fun{f_{X_1, \cdots, X_n}}{\conditbar{x_1, \cdots, x_n}{\theta}} &= \prod_{i=1}^n \fun{f_{X_i}}{\conditbar{x_i}{\theta}} \\
    &= \prod_{i=1}^n \fun{\chi_{[i\theta, \infty)}}{x_i} e^{i\theta - x_i} \\
    &= \fun{\chi_{\left[ \theta, \infty \right)}}{\fun{\min_{1 \leq i \leq n}}{x_i}} \cdot e^{\displaystyle \sum_{i=1}^n i\theta - x_i} \\
    &= \underbrace{\fun{\chi_{\left[ \theta, \infty \right)}}{\fun{\min_{1 \leq i \leq n}}{x_i}} e^{in\theta}}_{\fun{g}{\conditbar{\fun{\displaystyle \min_{1 \leq i \leq n}}{x_i}}{\theta}}} \cdot \underbrace{e^{\displaystyle -\sum_{i=1}^n x_i}}_{\fun{h}{x}}.
\end{align*}
The result then follows from  \cite[Factorization Theorem 6.2.6 on page 276]{Berger-Casella}.

\newpage
\section{Exercise 6.3}

The joint distribution is given by

\begin{align*}
    \fun{f_{X_1, \cdots, X_n}}{\conditbar{x_1, \cdots, x_n}{\mu, \sigma}} &= \prod_{i=1}^n \fun{\chi_{\left( \mu, \infty \right)}}{x_i} \frac{1}{\sigma} e^{\frac{-(x_i - \mu)}{\sigma}} \\
    &= \underbrace{\fun{\chi_{\left( \mu, \infty \right)}}{\min_{1 \leq i \leq n} x_i} \cdot \frac{1}{\sigma^n} e^{\displaystyle \frac{\displaystyle -\sum_{i=1}^n x_i + n \mu}{\sigma}}}_{\fun{g}{\displaystyle \conditbar{\min_{1 \leq i \leq n} x_i, \sum_{i=1}^n x_i}{\mu, \sigma}}} \cdot \underbrace{1}_{\fun{h}{x}}.
\end{align*}
\cite[Factorization Theorem 6.2.6 on page 276]{Berger-Casella} says the pair $\left(\min_{1 \leq i \leq n} X_i, \sum_{i=1}^n X_i \right)$ gives a sufficient statistic for $(\mu, \sigma)$.

\newpage
\section{Exercise 6.5}

The joint distribution is given by

\begin{align*}
    \fun{f}{\conditbar{x_1, \cdots, x_n}{\theta}} &= \prod_{i=1}^n \fun{\chi_{\left( -i (\theta - 1), i (\theta + 1) \right)}}{x_i} \frac{1}{2i\theta} \\
    &= \prod_{i=1}^n \fun{\chi_{\left( \theta - 1, \theta + 1 \right)}}{\frac{x_i}{i}} \frac{1}{2i\theta} \\
    &= \fun{\chi_{(\theta - 1, \theta + 1)}}{\min_{1 \leq i \leq n} \frac{x_i}{i}} \cdot \fun{\chi_{(\theta - 1, \theta + 1)}}{\max_{1 \leq i \leq n} \frac{x_i}{i}} \cdot \frac{1}{(2\theta)^n n!}.
\end{align*}
\cite[Factorization Theorem 6.2.6 on page 276]{Berger-Casella} says the pair $\left(\min_{1 \leq i \leq n} \frac{X_i}{i}, \sum_{i=1}^n \frac{X_i}{i} \right)$ gives a sufficient statistic for $\theta$.

\newpage
\section{Exercise 6.6}
The joint distribution is given by

\begin{align*}
    \fun{f_{X_1, \cdots, X_n}}{\conditbar{x_1, \cdots, x_n}{\alpha, \beta}} &= \prod_{i=1}^n \frac{x_i^{\alpha - 1} e^{-\beta x_i} \beta^{\alpha}}{\fun{\Gamma}{\alpha}} \\
    &= \left( \frac{\beta^{\alpha}}{\fun{\Gamma}{\alpha}} \right)^n \cdot \left( \prod_{i=1}^n x_i \right)^{\alpha - 1} \cdot e^{\displaystyle - \beta \sum_{i=1}^n x_i}.
\end{align*}
\cite[Factorization Theorem 6.2.6 on page 276]{Berger-Casella} says the pair $\left(\prod_{i=1}^n X_i, \sum_{i=1}^n X_i \right)$ gives a sufficient statistic for $(\alpha, \beta)$.

\newpage
\section{Exercise 6.7}

The joint distribution is given by

\begin{align*}
    \fun{f_{X_1, Y_1, \cdots, X_n, Y_n}}{ \conditbar{x_1, y_1, \cdots, x_n, y_n}{\theta_1, \theta_2, \theta_3, \theta_4}} &= \prod_{i=1}^n \frac{1}{(\theta_4 - \theta_2)(\theta_3 - \theta_1)} \cdot \fun{\chi_{(\theta_1, \theta_3)}}{x_i} \cdot \fun{\chi_{\theta_2, \theta_4}}{y_i} \\
    &= \frac{1}{{(\theta_4 - \theta_2)^n (\theta_3 - \theta_1)^n}} \cdot\fun{\chi_{(\theta_1, \theta_3)}}{\min_{1 \leq i \leq n} x_i} \cdot \fun{\chi_{(\theta_1, \theta_3)}}{\max_{1 \leq i \leq n} x_i} \cdot \\
    &\hspace{2.5cm} \fun{\chi_{(\theta_2, \theta_4)}}{\min_{1 \leq i \leq n} y_i} \cdot \fun{\chi_{(\theta_2, \theta_4)}}{\max_{1 \leq i \leq n} y_i}
\end{align*}
\cite[Factorization Theorem 6.2.6 on page 276]{Berger-Casella} says the tuple $\left( \min_{1 \leq i \leq n} X_i, \max_{1 \leq i \leq n} X_i, \min_{1 \leq i \leq n} Y_i, \max_{1 \leq i \leq n} Y_i \right)$ gives a sufficient statistic for $(\theta_1, \theta_2, \theta_3, \theta_4)$.

\newpage
\section{Exercise 6.8}

We want to show the ordered statistic $(X_{(1)}, \cdots, X_{(n)})$ is a minimal sufficient statistic for $\theta$. The joint distribution is given by

\begin{align*}
    \fun{f_{X_1, \cdots, X_n}}{\conditbar{x_1, \cdots, x_n}{\theta}} = \prod_{i=1}^n \fun{f}{x_i - \theta}.
\end{align*}
If the tuples $(x_i)_{i=1}^n$, $(y_i)_{i=1}^n$ are two sample points, such that $\fun{T}{(x_i)_{i=1}^n} = \fun{T}{(y_i)_{i=1}^n}$, then the two sets

\[ \SETT{x_i}_{i=1}^n = \SETT{y_i}_{i=1}^n \]
are the same. Hence,

\[ \fun{f_{X_1, \cdots, X_n}}{\conditbar{x_1, \cdots, x_n}{\theta}} = \fun{f_{X_1, \cdots, X_n}}{\conditbar{y_1, \cdots, y_n}{\theta}}. \]
Therefore, \cite[Theorem 6.2.13 on page 281]{Berger-Casella} says the ordered statistic is minimal sufficient for $\theta$.

\newpage
\section{Exercise 6.9}

Refer to \cite[Theorem 6.2.13 on page 281]{Berger-Casella}.

\begin{enumerate}[label=(\alph*),leftmargin=*]
    \item The joint distribution is given by
    
    \begin{align*}
        \fun{f_{X_1, \cdots, X_n}}{\conditbar{x_1, \cdots x_n}{\theta}} &= \prod_{i=1}^n \Gaussianpdf{x_i-\theta}{} \\
        &= \frac{1}{(2 \pi)^{\frac{n}{2}}} \exp \left[ -\frac{1}{2} \sum_{i=1}^n (x_i- \theta)^2 \right].
    \end{align*}
    Therefore, we have
    
    \begin{align*}
        \frac{\fun{f_{X_1, \cdots, X_n}}{\conditbar{x_1, \cdots x_n}{\theta}}}{\fun{f_{X_1, \cdots, X_n}}{\conditbar{y_1, \cdots y_n}{\theta}}} &= \exp \left[ - \frac{1}{2} \sum_{i=1}^n (x_i - \theta)^2 - (y_i - \theta)^2 \right] \\
        &= \exp \left[ - \frac{1}{2} \sum_{i=1}^n x_i^2 - y_i^2 - 2\theta(y_i - x_i) \right] \\
        &= \exp \left[ - \frac{1}{2} \left( \sum_{i=1}^n x_i^2 - y_i^2 \right) + n \theta (\ol{y} - \ol{x}) \right],
    \end{align*}
    which is independent of $\theta$ if $\ol{x} = \ol{y}$. Therefore, $\fun{T}{X} = \ol{X}$ is a minimal sufficient statistic for $\theta$.
    
    \item The joint distribution is given by
    
    \begin{align*}
        \fun{f_{X_1, \cdots, X_n}}{\conditbar{x_1, \cdots, x_n}{\theta}} &= \prod_{i=1}^n \fun{\chi_{(\theta, \infty)}}{x_i} \cdot e^{-(x_i - \theta)} \\
        &= \fun{\chi_{(\theta, \infty)}}{\min_{1 \leq i \leq n} x_i} \cdot e^{-n\ol{x}} \cdot e^{n \theta}.
    \end{align*}
    Therefore, we have
    
    \begin{align*}
        \frac{\fun{f_{X_1, \cdots, X_n}}{\conditbar{x_1, \cdots x_n}{\theta}}}{\fun{f_{X_1, \cdots, X_n}}{\conditbar{y_1, \cdots y_n}{\theta}}} &= \frac{\fun{\chi_{(\theta, \infty)}}{\displaystyle \min_{1 \leq i \leq n} x_i} \cdot e^{-n\ol{x}}}{\fun{\chi_{(\theta, \infty)}}{\displaystyle \min_{1 \leq i \leq n} y_i} \cdot e^{-n\ol{y}}},
    \end{align*}
    which is independent of $\theta$ if $\min_{1 \leq i \leq n} x_i = \min_{1 \leq i \leq n} y_i$. Therefore, $\fun{T}{X} = \min_{1 \leq i \leq n} X_i$ is a minimal sufficient statistic for $\theta$.
    
    \item The joint distribution is given by
    
    \begin{align*}
        \fun{f_{X_1, \cdots, X_n}}{\conditbar{x_1, \cdots, x_n}{\theta}} &= \prod_{i=1}^n \frac{e^{-(x_i-\theta)}}{\left(1 + e^{-(x_i-\theta)} \right)^2} \\
        &= \frac{e^{-n \ol{x}}}{ \left[\displaystyle \prod_{i=1}^n 1 + e^{-(x_i - \theta)} \right]^2}.
    \end{align*}
    Therefore, we have
    
    \begin{align*}
         \frac{\fun{f_{X_1, \cdots, X_n}}{\conditbar{x_1, \cdots x_n}{\theta}}}{\fun{f_{X_1, \cdots, X_n}}{\conditbar{y_1, \cdots y_n}{\theta}}} &= \frac{e^{n \ol{y}}}{e^{n \ol{x}}} \cdot \left[ \prod_{i=1}^n \frac{1 + e^{-(y_i - \theta)}}{1 + e^{-(x_i - \theta)}} \right]^2 \\
         &= \frac{e^{n \ol{y}}}{e^{n \ol{x}}} \cdot \left[ \prod_{i=1}^n \frac{1 + e^{-(y_{(i)} - \theta)}}{1 + e^{-(x_{(i)} - \theta)}} \right]^2
    \end{align*}
    which is independent of $\theta$ if the ordered statistics of $x$ and $y$ agree. Therefore, the ordered statistic is a minimal sufficient statistic for $\theta$.
    
    \item A computation similar to part (c) shows the ordered statistic is a minimal sufficient statistic for $\theta$.
    
    \item The joint distribution is given by
    
    \begin{align*}
         \fun{f_{X_1, \cdots, X_n}}{\conditbar{x_1, \cdots, x_n}{\theta}} &= \prod_{i=1}^n \frac{1}{2} e^{-\Abs{x_i-\theta}} \\
         &= \frac{1}{2^n} e^{\displaystyle -\sum_{i=1}^n \Abs{x_i - \theta}}
    \end{align*}
    Therefore, the ordered statistic gives a minimal sufficient statistic for $\theta$.
    
\end{enumerate}

\newpage
\section{Exercise 6.11}

C.f \cite[Section 3.5]{Berger-Casella}. We see that if $Z \sim \fun{f}{\conditbar{x}{0}}$, then the two variables

\[ Z + \theta \sim X \] 
are equal in distribution. It follows that the ordered statistics

\[ Z_{(i)} + \theta \sim X_{(i)} \]
of random samples from $X$ and $Z$ are equivalent for $1 \leq i \leq n$. Therefore,

\begin{align*}
    \left( Y_1, \cdots Y_{n - 1} \right) &= \left( X_{(n)} - X_{(1)}, \cdots, X_{(n)} - X_{(n - 1)} \right) \\
    &\sim \left( Z_{(n)} + \theta - X_{(1)} - \theta, \cdots, Z_{(n)} + \theta - Z_{(n - 1)} - \theta \right) \\
    &= \left( Z_{(n)} - Z_{(1)}, \cdots, Z_{(n)} - Z_{(n - 1)} \right).
\end{align*}
The joint distribution of the last tuple is a function of the joint distribution of $(Z_1, \cdots, Z_n)$, which is a function of $\fun{f}{\conditbar{x}{0}}$. In other words, the joint distribution of the last tuple does not depend on $\theta$, proving the claim as desired.
\newpage
\section{Exercise 6.12}

\begin{enumerate}[label=(\alph*),leftmargin=*]
    \item Firstly, to prove minimal sufficiency, we compute the joint distribution
    
    \begin{align*}
        \fun{f_{X,N}}{\conditbar{x,n}{\theta}} &= \fun{f_X}{\conditbar{x}{\theta, n}} \cdot \fun{f_N}{n} \\
        &= \binom{n}{x} \theta^x (1-\theta)^{n-x} \cdot p_n.
    \end{align*}
    This gives the quotient
    
    \begin{align*}
        \frac{\fun{f_{X,N}}{\conditbar{x,n}{\theta}}}{\fun{f_{X,N}}{\conditbar{y,m}{\theta}}} &= \frac{ \binom{n}{x} \theta^x (1-\theta)^{n-x} \cdot p_n}{\binom{m}{y} \theta^y (1-\theta)^{m-y} \cdot p_m} \\
        &= \frac{\binom{n}{x}p_n \theta^{x-y} (1-\theta)^{n-m+y-x}}{\binom{m}{y}p_m},
    \end{align*}
    which is independent of $\theta$ if $x = y$ and $n = m$. As a result, the tuple $(X, N)$ gives a minimal sufficient statistic for $\theta$.
    
    Next, $N$ is ancillary for $\theta$ because its distribution
    
    \begin{align*}
        \fun{f_N}{n} = p_n
    \end{align*}
    does not depend on $\theta$.
    
    \item For the uniase part, we need to show the expected values for $X/N$ and $\theta$ agree. We compute
    
    \begin{align*}
        \fun{E}{X/N} &= \fun{E}{\fun{E}{\conditbar{X/N}{N}}} \\
        &= \fun{E}{N^{-1} \fun{E}{\conditbar{X}{N}}} & \left( \mbox{c.f. solution to Exercise 4.30 (a)} \right) \\
        &= \fun{E}{N^{-1} \cdot \fun{E}{\binomdist{N, \theta}}} \\
        &= \fun{E}{N^{-1} \cdot N\theta} \\
        &= \fun{E}{\theta}.
    \end{align*}
    
    We now compute the variance with \cite[Theorem 4.4.7 page 167]{Berger-Casella}:
    
    \begin{align*}
        \Var{X/N} &= \fun{E}{\Var{\conditbar{X/N}{N}}} + \Var{\fun{E}{\conditbar{X/N}{N}}} \\
        &= \fun{E}{N^{-2} \Var{\conditbar{X}{N}}} + \Var{N^{-1} \fun{E}{\conditbar{X}{N}}} \\
        &= \fun{E}{N^{-2} \Var{\binomdist{N, \theta}}} + \Var{N^{-1} \fun{E}{\binomdist{N, \theta}}} \\
        &= \fun{E}{\frac{N\theta (1-\theta)}{N^2}} + \Var{\theta} \\
        &= \theta (1-\theta) \fun{E}{\frac{1}{N}}.
    \end{align*}
\end{enumerate}

\newpage
\section{Exercise 6.13}

From \cite[Theorem 4.3.5 on page 161]{Berger-Casella}, the variables$Y_i = \log(X_i)$ are iid observations from the pdf
\begin{align*}
    \fun{f_Y}{\conditbar{y}{\alpha}} &= \fun{f_{X}}{\conditbar{e^y}{\alpha}} \cdot \Abs{e^y} \\
    &= \alpha e^{\alpha y - e^{\alpha y}},
\end{align*}
In particular, if $\hat{Y} \sim \fun{f_Y}{\conditbar{y}{1}}$, then $Y \sim \frac{1}{\alpha} \hat{Y}$. Therefore,

\begin{align*}
    \frac{\log(X_1)}{\log(X_2)} &\sim \frac{Y_1}{Y_2} \\
    &\sim \frac{\hat{Y}_1}{\hat{Y}_2}.
\end{align*}
The distribution of the last quotient is a function of $\fun{f_Y}{\conditbar{y}{1}}$, which does not depend on $\alpha$, proving the claim as desired.

\newpage
\section{Exercise 6.14}

Fix $n \geq 1$, and write $MY$ the sample median computed from $n$ random sample from $X$.

Next, define the variable

\[ Z = \frac{X- EX}{\sqrt{\Var{X}}}. \]
Then $Z$ is in the location family, and does not depend on the location parameter. Moreover, we have

\[ X = Z + EX. \]
It follows that

\begin{align*}
    MX &= MZ + EX, \\
    \ol{X} &= \ol{Z} + EX.
\end{align*}
Hence, we obtain

\begin{align*}
    MX - \ol{X} &= MZ - \ol{Z},
\end{align*}
and the claim holds as desired.

\newpage
\nocite{*}
\printbibliography

\end{document}